<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vm on Shell&#39;s Home</title>
    <link>https://shell909090.github.io/tags/vm/</link>
    <description>Recent content in Vm on Shell&#39;s Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Feb 2014 10:14:23 +0800</lastBuildDate>
    <atom:link href="https://shell909090.github.io/tags/vm/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>利用cgroups隔离多个进程资源消耗的尝试</title>
      <link>https://shell909090.github.io/blog/archives/2561/</link>
      <pubDate>Wed, 19 Feb 2014 10:14:23 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/2561/</guid>
      <description>&lt;h1&gt;setup&lt;/h1&gt;

&lt;p&gt;Add to /etc/fstab.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cgroup                  /sys/fs/cgroup  cgroup  defaults        0       2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then &lt;code&gt;sudo mount -a&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Change directory to /sys/fs/cgroup/. Use mkdir to create a new group, and initalize it like those.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo 0-3 &amp;gt;&amp;gt; cpuset.cpus
echo 0 &amp;gt; cpuset.mems
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Without those two step, next operator will failure.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo 0 &amp;gt; memory.swappiness
echo 52428800 &amp;gt; memory.limit_in_bytes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set memory limit 50M, and forbidden swap.&lt;/p&gt;

&lt;h1&gt;python test code&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;import os, sys, time, random, string
def main():
    l = []
    random.seed()
    raw_input()
    for i in xrange(1000000):
        for j in xrange(1000):
            t = list(string.letters)
            random.shuffle(t)
            s = &#39;&#39;.join(t)
            l.append(s)
        time.sleep(0.01)

if __name__ == &#39;__main__&#39;: main()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, it triggered OOM.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>lxc和virtualbox和物理机的简单性能测试和对比</title>
      <link>https://shell909090.github.io/blog/archives/2542/</link>
      <pubDate>Thu, 23 Jan 2014 11:04:34 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/2542/</guid>
      <description>&lt;h1&gt;说明&lt;/h1&gt;

&lt;p&gt;测试各种虚拟化系统下的虚拟机性能。&lt;/p&gt;

&lt;p&gt;测试使用sysbench。&lt;/p&gt;

&lt;p&gt;CPU采用如下指令测试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysbench --test=cpu --num-threads=2 --cpu-max-prime=50000 run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;文件IO采用如下指令测试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysbench --test=fileio --file-total-size=10G prepare
sysbench --test=fileio --file-total-size=10G --file-test-mode=rndrw --init-rng=on --max-time=300 --max-requests=0 run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内存采用如下指令测试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysbench --test=memory --num-threads=2 --memory-access-mode=seq run
sysbench --test=memory --num-threads=2 --memory-access-mode=rnd run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;线程采用如下指令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysbench --test=threads --num-threads=2 run
sysbench --test=mutex --num-threads=2 --mutex-locks=1000000 run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;裸硬盘测试采用如下指令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hdparm -tT &amp;lt;dev&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;物理机上有三个文件系统，ext4/xfs/btrfs，前两者仅做fileio测试以对比性能。&lt;/p&gt;

&lt;p&gt;另外做两个特殊文件系统对比，aufs带复制和aufs无复制。前者在只读层上准备好测试文件，而后进行随机读写测试。其中就附带了文件复制开销。后者在aufs建立后初始化测试文件，因此消除了文件复制开销。&lt;/p&gt;

&lt;p&gt;所有测试都是测试数次，取最高者（因为低者可能受到各种干扰）。一般是2-3次。&lt;/p&gt;

&lt;p&gt;物理机是一台DELL Intel 64位桌面系统，支持硬件虚拟化，有4G内存。系统采用debian jessie，测试于2014年1月17日-20日执行，内核3.12.6-2 (2013-12-29) x86_64。&lt;/p&gt;

&lt;p&gt;虚拟机lxc是使用lxc切分的一台虚拟机，没有做资源限制。&lt;/p&gt;

&lt;p&gt;虚拟机vbox是使用virtualbox切分的一台虚拟机，分配了所有CPU，打开了硬件虚拟化，分配了1G内存。&lt;/p&gt;

&lt;h1&gt;文件系统&lt;/h1&gt;

&lt;h2&gt;ext4&lt;/h2&gt;

&lt;p&gt;Operations performed: 21311 Read, 14207 Write, 45440 Other = 80958 Total Read 332.98Mb Written 221.98Mb Total transferred 554.97Mb (1.8499Mb/sec) 118.39 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0044s total number of events: 35518 total time taken by event execution: 168.4761 per-request statistics: min: 0.00ms avg: 4.74ms max: 118.67ms approx. 95 percentile: 12.48ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 35518.0000/0.00 execution time (avg/stddev): 168.4761/0.00&lt;/p&gt;

&lt;h2&gt;xfs&lt;/h2&gt;

&lt;p&gt;Operations performed: 20789 Read, 13859 Write, 44288 Other = 78936 Total Read 324.83Mb Written 216.55Mb Total transferred 541.38Mb (1.8046Mb/sec) 115.49 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0018s total number of events: 34648 total time taken by event execution: 172.0475 per-request statistics: min: 0.00ms avg: 4.97ms max: 96.11ms approx. 95 percentile: 12.30ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 34648.0000/0.00 execution time (avg/stddev): 172.0475/0.00&lt;/p&gt;

&lt;h2&gt;btrfs&lt;/h2&gt;

&lt;p&gt;Operations performed: 6180 Read, 4120 Write, 13105 Other = 23405 Total Read 96.562Mb Written 64.375Mb Total transferred 160.94Mb (549.23Kb/sec) 34.33 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0556s total number of events: 10300 total time taken by event execution: 65.8914 per-request statistics: min: 0.00ms avg: 6.40ms max: 337.28ms approx. 95 percentile: 17.01ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 10300.0000/0.00 execution time (avg/stddev): 65.8914/0.00&lt;/p&gt;

&lt;h2&gt;aufs透明&lt;/h2&gt;

&lt;p&gt;Operations performed: 5340 Read, 3560 Write, 11279 Other = 20179 Total Read 83.438Mb Written 55.625Mb Total transferred 139.06Mb (474.65Kb/sec) 29.67 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0084s total number of events: 8900 total time taken by event execution: 32.3634 per-request statistics: min: 0.00ms avg: 3.64ms max: 1037.04ms approx. 95 percentile: 0.02ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 8900.0000/0.00 execution time (avg/stddev): 32.3634/0.00&lt;/p&gt;

&lt;h2&gt;aufs非透明&lt;/h2&gt;

&lt;p&gt;Operations performed: 20320 Read, 13546 Write, 43264 Other = 77130 Total Read 317.5Mb Written 211.66Mb Total transferred 529.16Mb (1.7638Mb/sec) 112.88 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0054s total number of events: 33866 total time taken by event execution: 170.7252 per-request statistics: min: 0.00ms avg: 5.04ms max: 143.86ms approx. 95 percentile: 12.62ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 33866.0000/0.00 execution time (avg/stddev): 170.7252/0.00&lt;/p&gt;

&lt;h1&gt;物理机&lt;/h1&gt;

&lt;h2&gt;hdparm&lt;/h2&gt;

&lt;p&gt;Timing cached reads: 11980 MB in 2.00 seconds = 5992.56 MB/sec Timing buffered disk reads: 366 MB in 3.01 seconds = 121.52 MB/sec&lt;/p&gt;

&lt;h2&gt;cpu&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 51.4463s total number of events: 10000 total time taken by event execution: 102.8828 per-request statistics: min: 9.93ms avg: 10.29ms max: 36.11ms approx. 95 percentile: 11.29ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/30.00 execution time (avg/stddev): 51.4414/0.00&lt;/p&gt;

&lt;h2&gt;memory&lt;/h2&gt;

&lt;p&gt;Operations performed: 104857600 (4545662.48 ops/sec)&lt;/p&gt;

&lt;p&gt;102400.00 MB transferred (4439.12 MB/sec)&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 23.0676s total number of events: 104857600 total time taken by event execution: 34.1991 per-request statistics: min: 0.00ms avg: 0.00ms max: 18.05ms approx. 95 percentile: 0.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 52428800.0000/69790.00 execution time (avg/stddev): 17.0995/0.01&lt;/p&gt;

&lt;p&gt;Operations performed: 104857600 (5407739.22 ops/sec)&lt;/p&gt;

&lt;p&gt;102400.00 MB transferred (5281.00 MB/sec)&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 19.3903s total number of events: 104857600 total time taken by event execution: 26.8579 per-request statistics: min: 0.00ms avg: 0.00ms max: 22.46ms approx. 95 percentile: 0.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 52428800.0000/7211.00 execution time (avg/stddev): 13.4289/0.14&lt;/p&gt;

&lt;h2&gt;threads&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 1.0112s total number of events: 10000 total time taken by event execution: 2.0210 per-request statistics: min: 0.15ms avg: 0.20ms max: 11.19ms approx. 95 percentile: 0.24ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/3.00 execution time (avg/stddev): 1.0105/0.00&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 0.1665s total number of events: 2 total time taken by event execution: 0.3238 per-request statistics: min: 157.39ms avg: 161.90ms max: 166.41ms approx. 95 percentile: 10000000.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 1.0000/0.00 execution time (avg/stddev): 0.1619/0.00&lt;/p&gt;

&lt;h2&gt;nginx&lt;/h2&gt;

&lt;p&gt;Concurrency Level: 10000 Time taken for tests: 5.745 seconds Complete requests: 100000 Failed requests: 0 Write errors: 0 Total transferred: 172100000 bytes HTML transferred: 160000000 bytes Requests per second: 17404.93 &amp;#91;#/sec&amp;#93; (mean) Time per request: 574.550 &amp;#91;ms&amp;#93; (mean) Time per request: 0.057 &amp;#91;ms&amp;#93; (mean, across all concurrent requests) Transfer rate: 29251.85 [Kbytes/sec] received&lt;/p&gt;

&lt;h1&gt;lxc&lt;/h1&gt;

&lt;h2&gt;cpu&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 51.4368s total number of events: 10000 total time taken by event execution: 102.8619 per-request statistics: min: 9.92ms avg: 10.29ms max: 35.08ms approx. 95 percentile: 11.68ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/5.00 execution time (avg/stddev): 51.4310/0.00&lt;/p&gt;

&lt;h2&gt;fileio&lt;/h2&gt;

&lt;p&gt;Operations performed: 5548 Read, 3698 Write, 11776 Other = 21022 Total Read 86.688Mb Written 57.781Mb Total transferred 144.47Mb (493.07Kb/sec) 30.82 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0294s total number of events: 9246 total time taken by event execution: 84.4687 per-request statistics: min: 0.01ms avg: 9.14ms max: 394.13ms approx. 95 percentile: 36.20ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 9246.0000/0.00 execution time (avg/stddev): 84.4687/0.00&lt;/p&gt;

&lt;h2&gt;memory&lt;/h2&gt;

&lt;p&gt;Operations performed: 104857600 (4456398.83 ops/sec)&lt;/p&gt;

&lt;p&gt;102400.00 MB transferred (4351.95 MB/sec)&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 23.5297s total number of events: 104857600 total time taken by event execution: 34.8417 per-request statistics: min: 0.00ms avg: 0.00ms max: 20.22ms approx. 95 percentile: 0.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 52428800.0000/155952.00 execution time (avg/stddev): 17.4208/0.06&lt;/p&gt;

&lt;p&gt;Operations performed: 104857600 (5327923.43 ops/sec)&lt;/p&gt;

&lt;p&gt;102400.00 MB transferred (5203.05 MB/sec)&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 19.6808s total number of events: 104857600 total time taken by event execution: 27.3010 per-request statistics: min: 0.00ms avg: 0.00ms max: 16.48ms approx. 95 percentile: 0.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 52428800.0000/297738.00 execution time (avg/stddev): 13.6505/0.09&lt;/p&gt;

&lt;h2&gt;threads&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 1.2490s total number of events: 10000 total time taken by event execution: 2.4954 per-request statistics: min: 0.21ms avg: 0.25ms max: 7.39ms approx. 95 percentile: 0.28ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/7.00 execution time (avg/stddev): 1.2477/0.00&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 0.1222s total number of events: 2 total time taken by event execution: 0.2275 per-request statistics: min: 107.53ms avg: 113.77ms max: 120.02ms approx. 95 percentile: 10000000.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 1.0000/0.00 execution time (avg/stddev): 0.1138/0.01&lt;/p&gt;

&lt;h2&gt;nginx&lt;/h2&gt;

&lt;p&gt;Concurrency Level: 10000 Time taken for tests: 16.976 seconds Complete requests: 100000 Failed requests: 0 Write errors: 0 Total transferred: 1551500000 bytes HTML transferred: 1539400000 bytes Requests per second: 5890.69 &amp;#91;#/sec&amp;#93; (mean) Time per request: 1697.594 &amp;#91;ms&amp;#93; (mean) Time per request: 0.170 &amp;#91;ms&amp;#93; (mean, across all concurrent requests) Transfer rate: 89252.02 [Kbytes/sec] received&lt;/p&gt;

&lt;h1&gt;vbox&lt;/h1&gt;

&lt;h2&gt;hdparm&lt;/h2&gt;

&lt;p&gt;Timing cached reads: 10122 MB in 1.99 seconds = 5088.70 MB/sec Timing buffered disk reads: 300 MB in 3.00 seconds = 99.87 MB/sec&lt;/p&gt;

&lt;h2&gt;cpu&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 54.0469s total number of events: 10000 total time taken by event execution: 108.0595 per-request statistics: min: 9.03ms avg: 10.81ms max: 61.39ms approx. 95 percentile: 14.87ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/87.00 execution time (avg/stddev): 54.0297/0.00&lt;/p&gt;

&lt;h2&gt;fileio&lt;/h2&gt;

&lt;p&gt;Operations performed: 68153 Read, 45435 Write, 145280 Other = 258868 Total Read 1.0399Gb Written 709.92Mb Total transferred 1.7332Gb (5.916Mb/sec) 378.62 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0045s total number of events: 113588 total time taken by event execution: 177.2314 per-request statistics: min: 0.01ms avg: 1.56ms max: 579.66ms approx. 95 percentile: 13.10ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 113588.0000/0.00 execution time (avg/stddev): 177.2314/0.00&lt;/p&gt;

&lt;h2&gt;memory&lt;/h2&gt;

&lt;p&gt;一直报错，测试不出来。&lt;/p&gt;

&lt;h2&gt;threads&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 16.0377s total number of events: 10000 total time taken by event execution: 32.0421 per-request statistics: min: 1.19ms avg: 3.20ms max: 38.39ms approx. 95 percentile: 5.74ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/15.00 execution time (avg/stddev): 16.0210/0.00&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 0.3023s total number of events: 2 total time taken by event execution: 0.5990 per-request statistics: min: 297.52ms avg: 299.50ms max: 301.47ms approx. 95 percentile: 10000000.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 1.0000/0.00 execution time (avg/stddev): 0.2995/0.00&lt;/p&gt;

&lt;h2&gt;nginx&lt;/h2&gt;

&lt;p&gt;Concurrency Level: 5000 Time taken for tests: 41.758 seconds Complete requests: 50000 Failed requests: 0 Write errors: 0 Keep-Alive requests: 0 Total transferred: 890350000 bytes HTML transferred: 884250000 bytes Requests per second: 1197.38 &amp;#91;#/sec&amp;#93; (mean) Time per request: 4175.799 &amp;#91;ms&amp;#93; (mean) Time per request: 0.835 &amp;#91;ms&amp;#93; (mean, across all concurrent requests) Transfer rate: 20821.94 [Kbytes/sec] received&lt;/p&gt;

&lt;h1&gt;vbox on lvm&lt;/h1&gt;

&lt;p&gt;在物理机上开辟一个lvm卷，然后用vmdk引用物理卷的功能挂到vbox上使用，格式化为ext4文件格式。&lt;/p&gt;

&lt;h2&gt;hdparm&lt;/h2&gt;

&lt;p&gt;Timing cached reads: 10034 MB in 1.99 seconds = 5044.30 MB/sec Timing buffered disk reads: 270 MB in 3.01 seconds = 89.73 MB/sec&lt;/p&gt;

&lt;h2&gt;fileio&lt;/h2&gt;

&lt;p&gt;Operations performed: 22765 Read, 15176 Write, 48512 Other = 86453 Total Read 355.7Mb Written 237.12Mb Total transferred 592.83Mb (1.976Mb/sec) 126.47 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0080s total number of events: 37941 total time taken by event execution: 286.5402 per-request statistics: min: 0.01ms avg: 7.55ms max: 336.67ms approx. 95 percentile: 20.49ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 37941.0000/0.00 execution time (avg/stddev): 286.5402/0.00&lt;/p&gt;

&lt;h1&gt;vbox on vdi&lt;/h1&gt;

&lt;p&gt;基本同vbox on lvm，不过使用vdi作为存储。和vbox测试相比，内部系统换为主系统同样的debian。&lt;/p&gt;

&lt;h2&gt;hdparm&lt;/h2&gt;

&lt;p&gt;Timing cached reads: 9152 MB in 1.99 seconds = 4598.80 MB/sec Timing buffered disk reads: 352 MB in 3.00 seconds = 117.16 MB/sec&lt;/p&gt;

&lt;h2&gt;fileio&lt;/h2&gt;

&lt;p&gt;Operations performed: 151020 Read, 100680 Write, 322060 Other = 573760 Total Read 2.3044Gb Written 1.5363Gb Total transferred 3.8406Gb (13.109Mb/sec) 839.00 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0011s total number of events: 251700 total time taken by event execution: 60.6296 per-request statistics: min: 0.01ms avg: 0.24ms max: 106.94ms approx. 95 percentile: 0.26ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 251700.0000/0.00 execution time (avg/stddev): 60.6296/0.00&lt;/p&gt;

&lt;h1&gt;分析&lt;/h1&gt;

&lt;h2&gt;计算&lt;/h2&gt;

&lt;p&gt;从CPU上说，vbox的消耗大约是5%，而lxc的消耗基本是0%。&lt;/p&gt;

&lt;p&gt;内存测试上，vbox无法测量。lxc的性能和物理机十分相近，两者的差异在1.5%-2%之间。&lt;/p&gt;

&lt;p&gt;在threads测试和mutex测试上，vbox显示出远远低于lxc的性能。这些基本都是内核陷入类的事务，也是预期vbox会发生性能下降的地方。&lt;/p&gt;

&lt;h2&gt;IO&lt;/h2&gt;

&lt;p&gt;从硬件设备IO上来分析，lxc和主机是共用一套物理设备的。vbox的裸设备IO比物理机低了15-18%。但是文件系统IO则表现出完全相反的景象。&lt;/p&gt;

&lt;p&gt;lxc底层使用的是btrfs，因此性能和物理机上的btrfs性能十分相近，误差在10%以内。这一性能比物理机上的ext4/xfs低了70%以上。这个表现出btrfs的性能和ext4/xfs性能的差异(注：怎么会差这么多？)。&lt;/p&gt;

&lt;p&gt;如果使用aufs的话，则要视复制特性而定。如果引发复制，性能会跌到和btrfs相近。而不引发复制的话，则和aufs下面的文件系统相近(误差在5%以内)。&lt;/p&gt;

&lt;p&gt;而vbox内只有ext4，其性能高达物理机的220%。怎么可能？&lt;/p&gt;

&lt;h2&gt;network&lt;/h2&gt;

&lt;p&gt;从nginx的rps来说，lxc的性能只有物理机的1/3，vbox的更只有7%。而且vbox连10000并发都无法支撑，只能用5000并发测试。&lt;/p&gt;

&lt;p&gt;这里固然有因为物理机目录中文件比较少的原因，但是vbox和lxc的文件数是一样的，两者性能比高达1:5是个不争的事实。&lt;/p&gt;

&lt;h2&gt;加测&lt;/h2&gt;

&lt;p&gt;因为vbox内的ext4性能太好，怀疑有鬼，所以加测了一下。果然，使用vdi后fileio性能比物理机还好（我用的是新的vdi）。这个结论在大规模读写和老的vdi上很可能退化成一点都不靠谱，否则所有的设备都应该先装一堆虚拟机做vdi。&lt;/p&gt;

&lt;h1&gt;结论&lt;/h1&gt;

&lt;h2&gt;虚拟化层级&lt;/h2&gt;

&lt;p&gt;从虚拟化的深度来说，CPU虚拟化最深，完全虚拟化次之，半虚拟化其后，硬件虚拟化和半虚拟化难分伯仲，操作系统虚拟化已经很浅了。下面是简单解说。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;CPU虚拟化：使用CPU仿真另一块CPU的每个指令。速度最慢（大约是真实CPU的1/60），可以仿真其他CPU。bochs为其代表，qemu亦支持。&lt;/li&gt;
&lt;li&gt;完全虚拟化：对另一个系统的部分核心调用进行处理。速度次之（大约一半略快）。必须是同种CPU，但可以是不同系统。vmware早期的虚拟机都是此种。&lt;/li&gt;
&lt;li&gt;半虚拟化：修改另一个系统的核心代码，使其与hypervisor交互以提高性能（大约5-10%）。必须同种CPU，但是guest系统必须可以修改（排除了windows）。Xen为其代表。&lt;/li&gt;
&lt;li&gt;硬件虚拟化：利用硬件加速完全虚拟化，使其性能接近半虚拟化，但是不需要修改内核。必须同种CPU，可以为不同系统。目前大部分虚拟机都支持。&lt;/li&gt;
&lt;li&gt;系统虚拟化：在系统上完成另一个系统的特性。必须是同种CPU同种内核，但可以是不同系统（例如debian和centos）。linux上的lxc/openvz，bsd上的jail，windows上的Virtuozzo为其代表。&lt;/li&gt;
&lt;li&gt;用户隔离：在同一个系统上，利用用户分割权限和限制配额。必须在同一个系统内。DAE为其代表。&lt;/li&gt;
&lt;li&gt;沙盒：在语言内部搭建隔离平台，对API进行鉴定和抽象。GAE为其代表。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;性能和隔离性的取舍&lt;/h2&gt;

&lt;p&gt;上面显然可见，隔离性越好，性能越差。使用硬件虚拟化后，nginx的rps只有原本的1/10。lxc的overload明明很低，但是因为用了虚拟网卡，所以rps下降到1/3。从效率上说，当然不希望为了虚拟化而消耗大量的资源。所以，如果服务原本可以用户隔离，就不要用系统虚拟化，如果可以系统虚拟化，就不要硬件虚拟化。所以在大规模使用虚拟化之前，不妨考虑一下是不是先好用户权限级隔离比较合适。&lt;/p&gt;

&lt;h2&gt;文件系统&lt;/h2&gt;

&lt;p&gt;从上面可以看出，对于lxc这种小消耗的虚拟方案，与其在意虚拟机的性能消耗，不如更在意文件系统的性能消耗。但是比较倒霉的是，lxc是不能在虚拟机内自行配置文件系统的，需要从主机内分配挂载。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ext4：适用于大部分情况，iops很高，小文件下吞吐量很不错。大部分虚拟化场景都是小系统简单服务的时候适合。&lt;/li&gt;
&lt;li&gt;xfs：适用于大设备内部的大虚拟机。每个虚机的服务复杂，或者是有大文件。&lt;/li&gt;
&lt;li&gt;btrfs：这个东西性能很低，但是写时复制的能力对快速clone很重要。适合用于产生一堆临时生成的环境，用于程序员测试或者测试工程师搭环境，测试完了就删除的。&lt;/li&gt;
&lt;li&gt;aufs：和btrfs情况差不多，快速clone不错。不过安全起见，clone后原image不可以启动实例或者修改image。在没有COW写入时性能很高，这点比btrfs好。而在COW时瞬时开销很高。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>lxc简单介绍</title>
      <link>https://shell909090.github.io/blog/archives/2533/</link>
      <pubDate>Thu, 02 Jan 2014 12:48:17 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/2533/</guid>
      <description>&lt;h1&gt;基本安装&lt;/h1&gt;

&lt;p&gt;安装lxc包。&lt;/p&gt;

&lt;p&gt;注意修改/bin/sh，链接到/bin/bash。lxc在某些版本上有一个bug，声明为/bin/sh却使用bash语法，导致不如此链接会出现错误。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wiki.debian.org/LXC&#34;&gt;lxc on debian wiki&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;镜像和设定&lt;/h1&gt;

&lt;p&gt;使用&lt;code&gt;lxc-create -n name -t template&lt;/code&gt;生成镜像。&lt;/p&gt;

&lt;p&gt;在/usr/share/lxc/templates可以看到可用的模板。&lt;/p&gt;

&lt;p&gt;在/var/cache/lxc/debian会缓存生成过程的临时文件。&lt;/p&gt;

&lt;p&gt;生成的镜像需要在镜像内安装lxc，否则无法使用lxc-execute。&lt;/p&gt;

&lt;h1&gt;资源限制&lt;/h1&gt;

&lt;p&gt;在config文件内，写入cgroup限定规则。注意，使用内存限定的话，需要在内核参数中加入cgroup_enable=memory。&lt;/p&gt;

&lt;p&gt;在debian下，可以修改/etc/default/grub文件，使用update-grub重新生成规则。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&#34;cgroup_enable=memory quiet&#34;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在config文件中可以如下限定。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc.cgroup.memory.limit_in_bytes = 512M # 限定内存
lxc.cgroup.cpuset.cpus = 0 # 限定可以使用的核
lxc.cgroup.blkio.throttle.read_bps_device = 8:0 100 # 读取速率限定
lxc.cgroup.blkio.throttle.write_bps_device = 8:0 100 # 写入速率限定
lxc.cgroup.blkio.throttle.read_iops_device = 8:0 100 # 读取频率限定
lxc.cgroup.blkio.throttle.write_iops_device = 8:0 100 # 写入频率限定
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/&#34;&gt;cgroups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/blkio-controller.txt&#34;&gt;blkio-controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/cpusets.txt&#34;&gt;cpusets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/memory.txt&#34;&gt;memory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;执行&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;lxc-start -n name /bin/echo hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以用以下指令，在已经启动的container里执行进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc-attach -n name /bin/echo hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下指令是用来在未启动的container里执行进程的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc-execute -n name /bin/echo hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;** 注意，虽然系统初始化了所有资源，但是由于sysv-init没有执行，因此系统内初始化并没完成。这导致系统不完整，例如网络部分不可用。 **&lt;/p&gt;

&lt;h1&gt;网络隔离&lt;/h1&gt;

&lt;h2&gt;自行设定的网络&lt;/h2&gt;

&lt;p&gt;lxc采用veth技术，每次虚拟机建立时，都会产生一对veth。虚拟机内的一般叫做eth0，虚拟机外的叫做vethXXX。这张网卡可以以任何linux许可的方式进行配置。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dnsmasq所产生的dhcp封包无法被debian guest的dhcp client承认，因此无法使用dnsmasq自动部署网络。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;使用iptables配置访问&lt;/h2&gt;

&lt;p&gt;在开启br-nf后，所有bridge的包都会经过iptables的过滤。因而可以用iptables的FORWARD规则限制guest堆外网的访问。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://serverfault.com/questions/162366/iptables-bridge-and-forward-chain&#34;&gt;bridge-nf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;参考&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;http://lxc.teegra.net/&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>lxc路由模式</title>
      <link>https://shell909090.github.io/blog/archives/2404/</link>
      <pubDate>Fri, 24 May 2013 09:25:36 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/2404/</guid>
      <description>&lt;h1&gt;为什么使用路由模式&lt;/h1&gt;

&lt;p&gt;lxc默认使用的是桥模式，这也是我在家里和公司里部署的模式。在这种模式下，lxc虚拟机可以直接和真实网络中的机器互相访问，就如同一台真的机器一样。路由模式则没有这个便利性。&lt;/p&gt;

&lt;p&gt;但是桥模式有个缺陷，必须能够做出桥来。我们有做不出桥来的时候么？有，如果你用笔记本，大部分AP会拒绝第二个MAC地址的包。导致网桥可以组建，却永远无法正常使用。&lt;/p&gt;

&lt;h1&gt;第一种路由模式，双重NAT简版&lt;/h1&gt;

&lt;p&gt;双重NAT可以用于几乎所有场景，并且不会带来后遗症。然而，双重NAT的问题在于，物理网络不能直接访问虚拟机。对于很多设备来说，这就失去了价值。另外说一点，之所以叫做双重NAT，是因为多数时候物理网络接到外网还需要一次NAT。&lt;/p&gt;

&lt;p&gt;lxc的双重NAT可以视为两步，建立NAT连接的虚拟网络，将lxc连接到虚拟网络。&lt;/p&gt;

&lt;p&gt;第一步比较复杂，我们先从br0的建立开始说起。首先，你需要为虚拟网络分配一个不同的保留内网网段。如果使用同样的内网网段，在ARP查询的时候会从一个端口发出超过一个的MAC回应，这就退回了桥模式。&lt;/p&gt;

&lt;p&gt;然后我们需要建立一个br0网桥作为配置的起点，对这个网桥赋予IP，配置路由和防火墙，并启动dnsmasq以便于dhcp和dns。这个模式之所以叫做简版，是因为我们先不讨论dnsmasq。&lt;/p&gt;

&lt;p&gt;假如你的lxc内网网段是192.168.66.0/24，那么你大致可以如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brctl addbr br0
ifconfig br0 192.168.66.1
route add -net 192.168.66.0/24 dev br0
iptables -A INPUT -s 192.168.66.0/24 -j ACCEPT
iptables -t nat -A POSTROUTING -s 192.168.66.0/24 -j MASQUERADE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上，对于任何一种网口设备，将其配置为NAT的过程都是一样的。&lt;/p&gt;

&lt;p&gt;第二步非常容易，在lxc的config文件内，指定网桥为br0就OK了。当然，作为略去dnsmasq的代价，你需要手工配置每台机器的IP地址和DNS服务器。&lt;/p&gt;

&lt;h1&gt;第二种路由模式，双重NAT&lt;/h1&gt;

&lt;p&gt;双重NAT的完整版需要在内网网口上启动dns，作为dns缓存代理和dhcp服务器。其余和第一种模式没有区别，只是你不需要手工指定IP和DNS服务器了。&lt;/p&gt;

&lt;p&gt;当然，其实任何一种网口的NAT配置都是一样的。&lt;/p&gt;

&lt;h1&gt;第三种路由模式，双边交互路由&lt;/h1&gt;

&lt;p&gt;第三种路由模式的效果最好，虚拟机和真实机可以互相访问。但是这种模式需要能够修改物理网络网关的路由表。这种模式使用主机作为路由器，中转真实网络和虚拟网络。&lt;/p&gt;

&lt;p&gt;我略去如何创造br以及如何将lxc连接到上面，这些前面有叙述。下面我简述一下双边路由最关键的几点。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;最重要的重点，就是在真实网络的网关上，将你的真实物理机在外网的IP，配置为虚拟网络的下一跳网关。例如，对于上面的例子，我们应当在网关上如此配置。&lt;/p&gt;

&lt;p&gt;route add -net 192.168.66.0/24 gw 192.168.1.4&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果不进行如此配置，物理网络所发出的包在到达网关后就不知道应当如何转发了。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在物理机上允许双边网络的所有包透过。你的包当然不能被防火墙挡掉。&lt;/li&gt;
&lt;li&gt;虚拟网络的dhcp是不会传递到外网的，因此如果打算使用dhcp，还是需要开dnsmasq。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>家庭电脑的虚拟化</title>
      <link>https://shell909090.github.io/blog/archives/2234/</link>
      <pubDate>Fri, 31 Aug 2012 16:39:38 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/2234/</guid>
      <description>&lt;p&gt;家庭电脑，谁都会用。会来看我blog的人更应当是家里有一台，我知道有些还有不止一台的——别人家我不知道，我家里就算老妈和丈母娘一起来打游戏，我还能保证我和老婆人手一台的水平。
一堆机器，有好处也有坏处。好处是，基本坏掉哪台都不怕，备用的比较多，随便来一台就能跑。坏处是，这些机器的配置不同，习惯不同，性能也不同。我们家里更特殊的情况是——连系统还不一样。我自己用的是linux，老婆是win7，老妈是XP。
为了解决文件共享的问题，我采用了NAS，而且是自己组装的小型服务器。对于小型家庭网络，NAS是个很不错的主意。然而电脑不仅仅有文件而已，还有配置呢。老婆的win7是直接连接到电视上的，所以我经常需要和她抢电脑。然而chromium的绑定gmail只能有一个——用我的还是她的就是一个问题，这是两个人用一台电脑的配置共享问题。同时，我的小上网本则是另一个极端。我希望上网本上和主机能共享同一个配置，虽然chromium的同步能力很强，但是很多东西不是chromium能同步的掉的。包括emacs配置，bookmark，打开文件。ssh密钥，系统环境。这是另一个问题，一个人用两台电脑的共享配置。当然，说到这里同时还有一个问题，我不希望用自己的小上网本，毕竟atom的速度和主机没法比，io速度也慢，内存也少。
所以，我最终的解决方案就是——虚拟化。在win7中装一台虚拟机，里面跑一个linux，再通过上网本远程控制这台linux，这样至少解决了我自己的问题。在小上网本上，可以高速的使用浏览器，和主机同一个配置。在主机上，和老婆分开配置。在老婆使用电脑的时候，和她分离的，不受干扰的使用电脑。
实际上，要解决这个问题，最好的方案是基于linux的multiseat系统。由于是multiseat，所以我和老婆同时使用。由于linux是用户分离的系统，所以可以互不干扰。唯一的遗憾是，同一个用户不能同时登录两个X，Xauthority文件会互相覆盖，因此在用户登录的情况下不能使用vnc。
当然，为什么不能用multiseat，你们懂。。。不懂的可以看我上一篇文章。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>kvm虚拟化的性能对比</title>
      <link>https://shell909090.github.io/blog/archives/2118/</link>
      <pubDate>Mon, 19 Mar 2012 07:09:38 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/2118/</guid>
      <description>&lt;p&gt;废话不多说，4G的机器，debian testing，amd64内核32位环境。用kvm切出一个768m的机器，debian testing，64位内核64位环境。对比起来有点不公平，不过基本说明问题。 &lt;div&gt;    用lmbench对比两者的性能，报告我就不贴了。基本结论如下：&lt;/div&gt;&lt;div&gt;1.纯计算: kvm内比真机还快，或者至少性能相当。估计是32位环境的关系。&lt;/div&gt;&lt;div&gt;2.syscall, read, write, select: 都是kvm快。&lt;/div&gt;&lt;div&gt;3.Protection fault, AF_UNIX sock stream latency, fork+exit, fork+execve: 真机比kvm快至少4倍，最多可达6倍。&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;div&gt;4.文件读写api: 真机比kvm快3倍以上。&lt;/div&gt;&lt;div&gt;5.socket性能: pipe是kvm快，unix file是真机快。&lt;/div&gt;&lt;div&gt;6.iozone: 两者吞吐性能几乎一致，有时kvm比真机还快。可能和真机的ext3上面堆满碎片结构有关。&lt;/div&gt;&lt;div&gt;    基本结论，kvm在计算和io上都没什么太大问题，主要问题在于各种涉及ring0指令的内核调用方面（好像这也是虚拟化的通病）。在虚拟化系统中，尽量避免大量的内核调用，尽量减少碎片调用，增大IO块。&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>有一种错误，叫做太常见了以至于视而不见</title>
      <link>https://shell909090.github.io/blog/archives/2098/</link>
      <pubDate>Mon, 20 Feb 2012 06:19:51 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/2098/</guid>
      <description>&lt;p&gt;最近整XEN，出了一个奇怪的错误。 &lt;div&gt;&lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;ERROR (SrvDaemon:355) Exception starting xend &lt;/span&gt;&lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;((22, &amp;#39;Invalid argument&amp;#39;)) &lt;/span&gt;&lt;br style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;Traceback (most recent call last): &lt;/span&gt;&lt;br style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt; &lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;   File &amp;quot;/usr/lib/xen-4.0/lib/python/xen/xend/server/SrvDaemon.py&amp;quot;, line &lt;/span&gt;&lt;br style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;335, in run &lt;/span&gt;&lt;br style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt; &lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;     xinfo = xc.xeninfo() &lt;/span&gt;&lt;br style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt; &lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;Error: (22, &amp;#39;Invalid argument&amp;#39;) &lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;    根据网络上的内容，首先排除没有xen模块——有了，然后是/proc/xen目录是否mount——有了，然后是/sys/hypervisor/下面的一堆属性——有了，然后是版本——不对。xen-utils-4.1是4.1.2版本，而xen-tools是4.2版本。不过xen-utils并不依赖xen-tools，没有后者应当也可以运行这些玩意。hypervisor和xen-utils的版本是一致的。那问题是什么？&lt;/span&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;div&gt;&lt;span style=&#34;font-size:13px;font-family:Verdana,Geneva,Helvetica,Arial,sans-serif&#34;&gt;    出错的文件是&lt;/span&gt;&lt;font face=&#34;Verdana, Geneva, Helvetica, Arial, sans-serif&#34;&gt;/usr/lib/xen-4.1/lib/python/xen/lowlevel/xc.so里面发生的异常，下载xen的源码检查，这个函数主要是检查属性。属性检查会出什么错？&lt;/font&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;div&gt;&lt;font face=&#34;Verdana, Geneva, Helvetica, Arial, sans-serif&#34;&gt;    正在一头雾水的时候，突然想起一个问题。我安装的是xen-hypervisor-4.1-amd64，因为kernel是linux-image-3.1.0-1-amd64。然而，我这个系统有一个非常大的特殊——在64位CPU上运行的32位系统。因此，实际上xen-utils是32位的。行了行了，下面的事情用膝盖都能想到。&lt;/font&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;div&gt;&lt;font face=&#34;Verdana, Geneva, Helvetica, Arial, sans-serif&#34;&gt;    叹气，这世界上，真的有种错误，叫做太常见了以至于视而不见。不要认为自己不会犯。&lt;/font&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>简易debian livecd打造手册</title>
      <link>https://shell909090.github.io/blog/archives/2094/</link>
      <pubDate>Thu, 16 Feb 2012 01:10:21 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/2094/</guid>
      <description>&lt;p&gt;    废话不说，上干货。先装一下syslinux，genisoimage，kvm，debootstrap，squashfs-tools。&lt;/p&gt; &lt;p&gt;$ mkdir debcd&lt;/p&gt; &lt;p&gt;$ cd debcd&lt;/p&gt; &lt;p&gt;$ mkdir isoroot&lt;/p&gt; &lt;p&gt;$ cp /usr/lib/syslinux/isolinux.bin isoroot/&lt;/p&gt; &lt;p&gt;$ cat &amp;gt; isoroot/isolinux.cfg &amp;lt;&amp;lt; &amp;quot;EOF&amp;quot;&lt;/p&gt; &lt;p&gt;prompt 0&lt;/p&gt; &lt;p&gt;default linux&lt;br /&gt;&lt;/p&gt; &lt;p&gt;label linux&lt;/p&gt; &lt;p&gt;    kernel vmlinuz&lt;/p&gt; &lt;p&gt;    append initrd=initrd.img&lt;/p&gt; &lt;p&gt;EOF&lt;/p&gt; &lt;p&gt;$ cp /boot/vmlinuz-3.2.0-1-amd64 isoroot/vmlinuz&lt;/p&gt; &lt;p&gt;    完成上述步骤后，你就准备好了一个基础的iso镜像文件系统，并有了一个基础的引导模块和内核。现在，我们尝试把这玩意烧到iso上，并且测试一下。&lt;/p&gt; &lt;p&gt;$ genisoimage -o output.iso -b isolinux.bin -c &lt;a href=&#34;http://boot.cat&#34;&gt;boot.cat&lt;/a&gt; -no-emul-boot -boot-load-size 4 -boot-info-table isoroot/&lt;/p&gt; &lt;p&gt;$ sudo kvm -cdrom output.iso -m 512&lt;/p&gt; &lt;p&gt;    如果没法装kvm，换成qemu。屏幕会停在内核引导过程中——因为你没有initrd.img，所以在isolinux.cfg中指定的initrd就不正确。下面我们会设法弄一个initrd.img。&lt;/p&gt; &lt;p&gt;$ cp -a /etc/initramfs-tools/ initramfs&lt;/p&gt; &lt;p&gt;$ mkinitramfs -d initramfs -o isoroot/initrd.img&lt;/p&gt; &lt;p&gt;$ genisoimage -o output.iso -b isolinux.bin -c &lt;a href=&#34;http://boot.cat&#34;&gt;boot.cat&lt;/a&gt; -no-emul-boot -boot-load-size 4 -boot-info-table isoroot/&lt;/p&gt; &lt;p&gt;$ sudo kvm -cdrom output.iso -m 512&lt;/p&gt; &lt;p&gt;    现在看看？你应该能看到有initrd被加载上去了，但是很可惜，没有root，因此也无法启动。所以下一步，我们需要弄一个root。&lt;/p&gt; &lt;p&gt;$ mkdir sysroot&lt;/p&gt; &lt;p&gt;$ sudo debootstrap --arch amd64 stable sysroot/ &lt;a href=&#34;http://localhost:9999/debian/&#34;&gt;http://localhost:9999/debian/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;$ sudo chown -R user.user sysroot&lt;/p&gt; &lt;p&gt;$ mksquashfs sysroot isoroot/rootfs -all-root&lt;/p&gt; &lt;p&gt;    把上面的源换成你喜欢的——我用approx做了一个缓存，所以一直使用这个缓存进行加速。在脚本执行完后，你会有一个压缩为squash格式的rootfs，可以作为root。但是这个root有两个缺陷。1.不能直接mount。2.即使mount了，启动的时候也会因为只读而挂掉。所以你需要做一点调整&lt;/p&gt; &lt;p&gt;$ cat &amp;gt;&amp;gt; initramfs/modules &amp;lt;&amp;lt; &amp;quot;EOF&amp;quot;&lt;/p&gt; &lt;p&gt;squashfs&lt;/p&gt; &lt;p&gt;aufs&lt;/p&gt; &lt;p&gt;EOF&lt;/p&gt; &lt;p&gt;$ cat &amp;gt; initramfs/scripts/local-premount/iso &amp;lt;&amp;lt; &amp;quot;EOF&amp;quot;&lt;/p&gt; &lt;p&gt;#!/bin/sh&lt;/p&gt; &lt;p&gt;case &amp;quot;${1}&amp;quot; in&lt;/p&gt; &lt;p&gt;    prereqs)&lt;/p&gt; &lt;p&gt;        echo &amp;#39;iso script run&amp;#39;&lt;/p&gt; &lt;p&gt;        exit 0&lt;/p&gt; &lt;p&gt;        ;;&lt;/p&gt; &lt;p&gt;esac&lt;/p&gt; &lt;p&gt;mkdir /cdrom&lt;/p&gt; &lt;p&gt;mount -t iso9660 /dev/sr0 /cdrom&lt;/p&gt; &lt;p&gt;mkdir /cdroot&lt;/p&gt; &lt;p&gt;mount -t squashfs /cdrom/rootfs /cdroot&lt;/p&gt; &lt;p&gt;mkdir /shadow&lt;/p&gt; &lt;p&gt;mount -t tmpfs -o size=128m none /shadow&lt;/p&gt; &lt;p&gt;mount -t aufs -o br:/shadow=rw:/cdroot=ro none /root&lt;/p&gt; &lt;p&gt;EOF&lt;/p&gt; &lt;p&gt;$ chmod +x initramfs/scripts/local-premount/iso&lt;/p&gt; &lt;p&gt;$ mkinitramfs -d initramfs -o isoroot/initrd.img&lt;/p&gt; &lt;p&gt;$ genisoimage -o output.iso -b isolinux.bin -c &lt;a href=&#34;http://boot.cat&#34;&gt;boot.cat&lt;/a&gt; -no-emul-boot -boot-load-size 4 -boot-info-table isoroot/&lt;/p&gt; &lt;p&gt;$ sudo kvm -cdrom output.iso -m 512&lt;/p&gt; &lt;p&gt;    好，现在再make clean，make test，光盘基本就OK了。&lt;/p&gt; &lt;p&gt;    root密码多少？我怎么知道你的root密码呢？用sudo chroot sysroot切换到自己的系统里面去改。另外，你可能需要安装一些软件，这时候记得把/sys /proc挂到chroot里面。还有记得调整一下/etc/udev/rules.d/70-persistent-net.rules，把主机里面的记录删掉（或者干脆删掉文件）。调整/etc/network/interfaces，把以下内容加进去。&lt;/p&gt; &lt;p&gt;auto lo&lt;/p&gt; &lt;p&gt;iface lo inet loopback&lt;br /&gt;&lt;/p&gt; &lt;p&gt;# The primary network interface&lt;/p&gt; &lt;p&gt;auto eth0&lt;/p&gt; &lt;p&gt;iface eth0 inet dhcp&lt;/p&gt; &lt;p&gt;    基本来说，可定制的引导系统就是这样。不过这个系统有以下几点需要注意：&lt;/p&gt; &lt;p&gt;1.可写入数量只有128M，如果写多了就完蛋。&lt;/p&gt; &lt;p&gt;2.128M全在内存中，内存不足完蛋。&lt;/p&gt; &lt;p&gt;3.一次一次生成很麻烦，我用的是make。&lt;/p&gt; &lt;p&gt;TARGETS=isoroot/initrd.img isoroot/rootfs&lt;br /&gt;&lt;/p&gt; &lt;p&gt;all: output.iso&lt;br /&gt;&lt;/p&gt; &lt;p&gt;test: output.iso&lt;/p&gt; &lt;p&gt;  sudo kvm -cdrom $^ -m 512&lt;br /&gt;&lt;/p&gt; &lt;p&gt;clean:&lt;/p&gt; &lt;p&gt;   rm -f output.iso $(TARGETS)&lt;br /&gt;&lt;/p&gt; &lt;p&gt;output.iso: isoroot/isolinux.cfg isoroot/vmlinuz $(TARGETS)&lt;/p&gt; &lt;p&gt;    genisoimage -o $@ -b isolinux.bin -c &lt;a href=&#34;http://boot.cat&#34;&gt;boot.cat&lt;/a&gt; -no-emul-boot -boot-load-size 4 -boot-info-table isoroot/&lt;br /&gt;&lt;/p&gt; &lt;p&gt;isoroot/initrd.img: initramfs&lt;/p&gt; &lt;p&gt;    mkinitramfs -d $^ -o $@&lt;br /&gt;&lt;/p&gt; &lt;p&gt;isoroot/rootfs: sysroot&lt;/p&gt; &lt;p&gt;    mksquashfs $^ $@ -all-root&lt;/p&gt; &lt;p&gt;4.可以用一个dsvc保存中间结果，我用的是git。下面是gitignore。&lt;/p&gt; &lt;p&gt;*.iso&lt;/p&gt; &lt;p&gt;isoroot/vmlinuz*&lt;/p&gt; &lt;p&gt;isoroot/initrd*&lt;/p&gt; &lt;p&gt;isoroot/rootfs*&lt;/p&gt; &lt;p&gt;sysroot&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>论同时的双系统－－准虚拟对双系统的进一步扩充</title>
      <link>https://shell909090.github.io/blog/archives/49/</link>
      <pubDate>Mon, 12 Jan 2009 01:04:00 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/49/</guid>
      <description>&lt;p&gt;熟悉贝壳的人都知道，贝壳是个linux爱用者，不过因为工作关系，经常要使用windows。贝壳在自己的笔记本上使用了linux/windows混合双系统，并通过共用磁盘的方式共享数据，解决这个问题。但是长期的使用表明，这种解决方案存在几个巨大的瑕疵。首先是系统切换时间常，因此长期在一个系统中工作，而很少触及另外一个系统。其次是稳定性差，windows下一旦崩溃，进入linux后就需要检测数据盘，80G的数据慢慢扫描，感觉晕到死。那么是否有一种方案，能够同时使用两个工作级系统（注意，不是实验级，贝壳成功的在windows下的vmware里跑了一个oracle，这个可以说是实验级的典范。然而工作级系统的要求和实验级完全不同）。
从系统发展史的角度来说，我们可以预见，将来的系统将是脱离硬件的。首要的原因就是和硬件不相匹配的各个层级的计算能力需求。现在系统发展有两个极端，一个是虚拟机，试图将一个硬件整体分离，运行多个系统。另一个是高性能集群，试图将多个硬件合并，运行一个系统。从根本上说，这是因为高性价比的硬件集中在了一个性能区间，而实际的性能需求却是完全分离的，因此我们才会出现如此两类完全背离的需求。而现在有大量宝贵的人力浪费在了系统和硬件结合，系统稳定性问题上，这无疑是对将来发展的一个巨大瓶颈。虽然无法预知将来的技术发展会以何种方式解决这个问题，然而可以预见的是，解决硬件和性能的背离将是人类计算机发展史上一个重要的里程碑，解决这个问题的人必定会在计算机历史上留下重重的一笔。
同时，更进一步，贝壳揣测，将来的解决方案将是系统硬件调度/驱动和系统软件管理分离。一个软系统拥有一个用户表和一个硬件表，硬件表上写他可能有10个键盘，两个显示器，或者一堆其他设备。系统借助某个可信方案，管理了一系列虚拟抽象设备和真实设备形成的映射。作为系统层以上的软件，我们只要关心如何操作这个虚拟设备即可。而实际上，我们可以通过管理参数和对应关系实现各种需要。例如我们可以将多个机器的硬件管理核心加入一个系统，形成集群。或者我们可以在一个机器的硬件管理核心上加入多个系统，形成虚拟机。这个基本是分布系统的观点。如此一来，系统层软件就无法得知也无需得知自己是在到底运行在什么环境下。只是这个系统设计方案对高性能要求的子系统（主要是显卡）相当不利。
从揣测回到现实，为了实现一个工作级系统（幸好，还不是工业级），我们需要为系统制定一些评判标准，以判别各个方案的优劣。我们首先能想到的评判标准就是速度，一个慢吞吞的系统解决方案是没有任何实用价值的。当然，这个速度是有差异的，可能是linux快一些，windows慢一些，或者相反。我们假定实际的需要是windows快一些，因为linux可以通过定制进行加速。
我们的第二个评判标准就是稳定性，经常会崩溃的系统不比慢吞吞的系统好到哪里去，甚至会更加让人讨厌。虽然工作级系统并没有工业级那样高的要求，然而高负荷稳定，宕机平均频率低于3天/次还是要保证的。而后我们还希望两个系统可以做到数据互通，即两个系统间的数据尽可能的共享，至少要做到文件和邮件的共享。最后，我们希望解决方案简单易用，便于实施和维护。
而后，我们列出了一个原始方案，和以下几个改进解决方案，并给出优劣评价，谨供大家参考借鉴。同时我们在其中还补充了一些无法实际解决问题的虚拟化解决方案，并且说明无法使用的原因，供适合的人自行选用。
原始方案，windows+linux+数据分区。此种方案是最中规中矩的，性能最高的方案。具有对硬件最好的支持，最容易的维护。如果需要运行游戏（尤其是魔兽，WOW），这也是唯一可行的工作级方案。稳定性评价属于尚可，主要由于ntfs在linux的稳定性并不好，ext3在windows需要使用非官方驱动，和某些（就是avast）驱动不兼容。数据互通比较方便，通过数据分区可以轻松的共享文件和邮件。
windows虚拟方案，vmware+虚拟分区。这种方案是改进方案中唯一可以跑游戏的，因为虚拟机随时可以关上。性能上满足windows快 linux慢的要求，虚拟系统显示性能良好，也可以通过文件共享部分的解决数据共享问题（文件共享方便，邮件共享困难）。稳定性很好，基本没有什么不稳定的问题出现，操作和维护都不困难。然而之所以一开始这种方案就被排除在外，主要是因为这种方案无法让linux驱动实体硬件，无法通过机器启动。这样也许对一些跑起来玩玩的人或者是内核工程师/测试员比较有用，然而如果要在linux里面进行大量工作，编译程序，运行服务，这种方案就力有未逮。因此这个方案可以说是一个实验级方案，而非工作级。
windows虚拟方案，vmware+实体硬盘。速度一般，windows快linux慢，基本和上面一个方案一样，唯一的区别就是linux也可以被实际驱动。然而这也成了整个方案的最大败笔，因为linux的驱动灵活性不如windows，因此无法经受这种系统切换的动作。举例来说，真实的机器上，硬盘是sata的，作为sda识别和使用。而虚拟机上则是IDE的，被识别成了hda。于是启动环境一变，就需要修改大量配置来调和这个问题。又例如，在真实机器上，X使用fglrx驱动，而虚拟机下面要用mesa。如果我在/etc/xorg.conf中不指定驱动，那么真实机器的驱动也会变成 mesa，导致性能下降。如果指定驱动，又会导致虚拟机内X无法运行。诸如此类的问题林林总总，需要大量细节修正，因此维护复杂，稳定性差，不建议正式使用。在贝壳机器上更严重的，出现了虚拟机内和虚拟机外争抢数据分区的状况，这种情况下数据分区实质是被当做盘阵用了。使用非专用的磁盘作为底层共享存储，并在上面运行ext3系统，这是及其危险和愚蠢的。
linux虚拟方案，xen。速度超快，但是上来就在贝壳的机器上暴出几个问题，因而没有继续测试。首先是安装xen后x无法启动，出现fglrx驱动无法加载的状况。其次是xen要求使用虚拟盘启动，可贝壳经常需要跑到windows下面打游戏。因此在简单测试后被剔除出局。感觉这种方案的最大问题在于配置管理太过复杂，debian下面已经很轻松了，只需要安装对应内核，使用工具建立虚拟机，但是依旧感觉麻烦到一塌糊涂。相信这种方案在专业级服务器领域应当有不俗表现。
linux虚拟方案，openvz。这种方案压根就不适合贝壳的状况，因为这个虚拟方案要求宿主和客户必须是同一CPU同一系统（不要求同一linux发行）。主要用于希望将一个主机切分成多个独立的同构主机，以达到分离管理的目的（例如业务服务器和数据库服务器分离）。需要做大型网络管理/虚拟主机业务的人可能会对这个虚拟方案感兴趣。
linux虚拟方案，vmware。速度一般，linux快windows慢，视频效果不错。vmware毕竟是商业公司，视频驱动挺齐全的。但是内核驱动的编译麻烦到死，首先是要求编译器版本和主内核编译器版本一致，于是贝壳去搞了个gcc-4.1，然后连接了上去。下面又是内核头定义出现版本差异，搞到现在还没有搞定。谁能搞的定的给个参考，最好是debian上的解决方案。
linux虚拟方案，kvm。这个是贝壳目前使用的方案，基本比较理想。速度很快，和xen基本差不多，显示速度不如vmware（理论上说装好显卡驱动应该会好点，不过贝壳找不到CLDC5446的XP驱动，那是Win32和Win95时代的显卡）。linux快windows慢，但是还在可忍受范围内。稳定性很好，只要测试通过，运行中到目前为止没有死机（当然很多参数是加了之后开机即死机）。数据可以通过samba互通，邮件也同样可以互通。然而使用samba无疑复杂很多，而且性能并不太好。只是从稳定性上说，让linux自己去驱动ext3总比半吊子的windows驱动更好，同时也不会出现争抢的问题。易用性上还算可以，无论是内核编译还是系统使用都不太难，最大的麻烦就是网络配置。根据贝壳的测试，在真实机器上superpi运行100W 位需要45秒，虚拟机内需要54-60秒，尤其在换用kvm-72.2后反而更慢了（54下降到60，折合真实机器83.3％下降到75％）。
总体来说，贝壳更倾向于使用全开源的准-全虚拟解决方案kvm，主要因为他简便易行，对系统影响小，不改变现有系统。同时性能高，稳定性好。主要需要解决显卡效率问题。如果以上问题无法彻底解决，贝壳打算换用linux下的vmware，想办法搞定他的内核模块。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux下的模拟器</title>
      <link>https://shell909090.github.io/blog/archives/395/</link>
      <pubDate>Wed, 27 Jun 2007 18:25:39 +0800</pubDate>
      
      <guid>https://shell909090.github.io/blog/archives/395/</guid>
      <description>&lt;p&gt;模拟器是一个很模糊的概念，究竟什么是模拟器？这个问题可能对于诸多玩友并不困难，但是对于程序员却是很难界定。什么是模拟器，bochs算吗？wine算吗？POSIX子系统算吗？OS/360算吗？&lt;br /&gt;    下面所定义的模拟器是至少具备以下几个特征的。&lt;br /&gt;    1.模拟目标机的CPU。按照这个特征，wine就被剔除出模拟器的范畴。这种东西其实最好规划在模拟子系统中(虚拟机)，这类软件是以本地CPU真实运行为基础特征的。如果这样被算入模拟器，那Windows算不算？&lt;br /&gt;    2.模拟目标机的硬件响应。这个特性其实说了和没说一样的。&lt;br /&gt;    限于贝壳接触的限制，目前我们的目标系统仅仅涵盖以下几种机器。GB/GBA NES/SNES(FC/SFC) NeoGeo MD 街机 PS。这几种机器相信应该没有人不知道吧。其中FC就是中国风靡一时的红白机。&lt;br /&gt;    我们来看看对应的模拟器。注意以下全是Linux系统下的模拟器，FreeBSD之类的需要进一步测试。&lt;br /&gt;    gngb&lt;br /&gt;        只能用于GB，GBA无法模拟。&lt;br /&gt;    gnuboy&lt;br /&gt;        和gngb看不出什么区别。&lt;br /&gt;    Visual Boy advance&lt;br /&gt;        至少从名字上知道能模拟GBA，不过我没有用，下面会说原因。&lt;br /&gt;    fceu&lt;br /&gt;        FC模拟器，非常好用的东西，有Windows版。除了吞食天地2外还没有模拟不出来的东西(贝壳语：为什么是我喜欢的吞食天地啊～～～)。不过贝壳一样没用，下面有原因。&lt;br /&gt;    mednafen&lt;br /&gt;        万能的救世主，最全能的模拟器(Linux下)。支持GB/GBA NES NeoGeo涵盖除了MD外的大多数系统，开源而且方便好用，具备Win32版本。不过吞食天地2一样模拟不出来。(贝壳：为什么～～～)&lt;br /&gt;    mopher&lt;br /&gt;        严格来说这不是Linux模拟器，而是WinCE的。不过鉴于一样是偏门系统，贝壳就顺便介绍以下好了，是GB/GBA NES/SNES MD的全能模拟器。&lt;br /&gt;    dgen&lt;br /&gt;        唯一的，也是最好的MD模拟器，可惜在AMD64系统上运行不大正常。&lt;br /&gt;    mame&lt;br /&gt;        就是Windows下超强模拟器mame的Linux版本，唯一能够模拟街机的模拟器。发布版本超多，支持Windows, Mac, Linux, Xbox(贝壳：?!), CE(贝壳：??!!), Nokia9210(贝壳：???!!!)。简直是模拟器族啊——！&lt;br /&gt;    pcsx&lt;br /&gt;        PS模拟器，其实是PS2啦。支持Windows, Linux, DreamCast(表问贝壳最后一个是啥东东)。如果你没有超级强劲的CPU就想都别想。&lt;br /&gt;    X GL SDL问题&lt;br /&gt;    这三者都是图形界面接口。一般来说，Xv是2D最快的，GL是3D最快的。所以能用X的不用SDL，能用SDL的不用GL，跑3D没的商量。&lt;br /&gt;    建议大家安装一个mednafen的X版本，一个mame的X版本。不是AMD64的装一个dgen，CPU够劲的装一个pcsx。基本上面的机器都能模拟了&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>