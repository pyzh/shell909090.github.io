<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lxc on Shell&#39;s Home</title>
    <link>http://shell909090.org/tags/lxc/</link>
    <description>Recent content in Lxc on Shell&#39;s Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2014 12:32:08 +0800</lastBuildDate>
    <atom:link href="http://shell909090.org/tags/lxc/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>docker的原理和类比</title>
      <link>http://shell909090.org/blog/archives/2650/</link>
      <pubDate>Mon, 30 Jun 2014 12:32:08 +0800</pubDate>
      
      <guid>http://shell909090.org/blog/archives/2650/</guid>
      <description>&lt;h1&gt;从虚拟化的种类和层级说起&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;cpu虚拟化：可以模拟不同CPU，例如bochs&lt;/li&gt;
&lt;li&gt;完全虚拟化：只能模拟同样CPU，但是可以执行不同系统，例如vmware&lt;/li&gt;
&lt;li&gt;半虚拟化：guest必须打补丁，例如Xen&lt;/li&gt;
&lt;li&gt;硬件虚拟化：可以当作获得硬件加速的完全虚拟化&lt;/li&gt;
&lt;li&gt;系统虚拟化：host和guest共享一样的内核，例如Openvz&lt;/li&gt;
&lt;li&gt;语言沙盒：只能在语言的范围内使用&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;虚拟化的级别越偏底层，速度越慢，用户越难察觉到虚拟化的存在。
虚拟化的级别越偏上层，速度越快，用户越容易感知。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cpu虚拟化和完全虚拟化时，用户几乎可以不察觉到虚拟化的存在&lt;/li&gt;
&lt;li&gt;半虚拟化时，guest内核必须存在补丁&lt;/li&gt;
&lt;li&gt;系统虚拟化时，用户不能控制自己的内核&lt;/li&gt;
&lt;li&gt;语言沙盒时，用户没有使用api的自由&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;docker的实现结构&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;docker

&lt;ul&gt;
&lt;li&gt;lxc&lt;/li&gt;
&lt;li&gt;namespace: 仅沙盒隔离，不限制资源。&lt;/li&gt;
&lt;li&gt;cgroup: 仅限制资源，不沙盒隔离。&lt;/li&gt;
&lt;li&gt;aufs&lt;/li&gt;
&lt;li&gt;image管理&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然，还有很多细节的东西，里面就不一一列举了。例如veth。&lt;/p&gt;

&lt;h2&gt;docker不是虚拟机&lt;/h2&gt;

&lt;p&gt;docker不是虚拟机，因为lxc已经是虚拟机。如果两者功能一样，那么docker就没有存在的必要。&lt;/p&gt;

&lt;p&gt;你可以把docker当虚拟机用，但是当虚拟机用的话，他的完备程度远远不及现在的种种虚拟机。相比之下，就会觉得很不好用。这不是docker的错，只能说被不正确的使用了。&lt;/p&gt;

&lt;h2&gt;docker是什么&lt;/h2&gt;

&lt;p&gt;docker就是环境。&lt;/p&gt;

&lt;p&gt;docker实际上只做了一件事情——镜像管理。负责将可执行的镜像导入导出，在不同设备上迁移。&lt;/p&gt;

&lt;p&gt;原本我们发布软件有两种方法，源码发布和二进制发布。二进制发布又有两种方案，静态链接和动态链接。最早的时候，我们发布软件都喜欢动态链接，因为小。但是随着网络和存储的升级，软件越来越喜欢静态链接，或者把动态库打包到发布里。因为系统情况越来越复杂，依赖关系一旦出错，系统就无法启动。&lt;/p&gt;

&lt;p&gt;将这个思路推到极限，就是虚拟机发布。早些年有人发过一些Oracle的linux安装镜像，算的上是先驱。因为Oracle早些年的安装程序很难用，对系统的依赖复杂。公司做测试用装一套Oracle还不够麻烦的。相比起来，下载一个虚拟机直接跑起来就可以用就方便了很多。即使性能差一些，测试而已也不是特别在意。&lt;/p&gt;

&lt;p&gt;docker再进了一步。不但提供一个镜像，可以在系统间方便的迁移。而且连镜像的升级都能做掉。更爽的是，升级只用传输差量数据。当然，有好处就有牺牲。&lt;/p&gt;

&lt;h2&gt;docker的镜像是只读的&lt;/h2&gt;

&lt;p&gt;其实不是，docker的镜像当然可以写入。但是写的时候有几个问题。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果对镜像进行写入，aufs会将原始文件复制一次，再进行写入。这样性能比较低。&lt;/li&gt;
&lt;li&gt;更直接的问题是，一旦对镜像做了写入，就无法从docker这里获得更新支持——docker不能将你的写入和上游的更新合并。因此，整个系统就退化成了一个完全的虚拟机。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以，我个人认为，docker的镜像本身应当是只读的——如同EC2里面一样。数据的写入应当通过远程文件系统或者数据库服务来解决。&lt;/p&gt;

&lt;h1&gt;vagrant&lt;/h1&gt;

&lt;p&gt;提到镜像管理，我们可以提一下同样属于镜像管理的一个软件——vagrant。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;可以将vbox的镜像打包导入导出&lt;/li&gt;
&lt;li&gt;提供了一个cloud，允许镜像的分享/更新&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;为什么vagrant不如docker出名&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;快，系统级虚拟化使得docker的虚拟化开销降低到百分级别以下。&lt;/li&gt;
&lt;li&gt;可以在虚拟机内使用的虚拟机，例如云主机内。&lt;/li&gt;
&lt;li&gt;资源调度灵活，不需要将资源预先划定给不同的实例，在不同资源的机器上也不用调整参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;成功案例&lt;/h1&gt;

&lt;h2&gt;编译系统/打包系统/集成测试环境&lt;/h2&gt;

&lt;p&gt;典型的搭建一次，执行一次，销毁一次。不需要对image做更改（准确说的需要做更改，但是不需要保存）。&lt;/p&gt;

&lt;h2&gt;公司内部应用&lt;/h2&gt;

&lt;p&gt;在IaaS的比拼中，以Openvz为代表的系统化虚拟化方案几乎完败于完全虚拟化/半虚拟化系列技术。就我和朋友的讨论，这里面最主要的因素在于。完全虚拟化技术可以比较好的隔离实例和实例间的资源使用，而系统虚拟化技术更偏向于将资源充分利用。这使得系统虚拟化更容易超售。&lt;/p&gt;

&lt;p&gt;然而，在公司内部应用中，这一缺陷就变成了优势。企业的诸多系统，只要在同一个优先级，其可用性应当是一致的。几个联动系统中，一个资源不足陷于濒死的情况下，保持其他几个系统资源充足并无意义。而且总资源是否足够应当是得到充分保证的事情，企业自己“超售”自己的资源，使得业务系统陷入运行缓慢的境地一点意义都没有。&lt;/p&gt;

&lt;p&gt;因此，系统虚拟化可以为企业级云计算提供可以灵活调度的资源，和非常低的额外开销。&lt;/p&gt;

&lt;p&gt;当然，云计算在企业化中原本就面临一些问题。原本提供软-硬件统一解决方案的集成商，需要如何重新组织解决方案。如何协调节约资源和高性能，高可用。云计算在企业级应用中还有很长的路要走。&lt;/p&gt;

&lt;h1&gt;短板&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;太新。目前成功案例还是不足，而且围绕docker的工具链还不完备。&lt;/li&gt;
&lt;li&gt;适用范围比较窄。需求需要集中在“环境迁移”领域，而且image本身不应被写入。&lt;/li&gt;
&lt;li&gt;生不逢时。rvm和virtualenv已经在前面了。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>lxc和virtualbox和物理机的简单性能测试和对比</title>
      <link>http://shell909090.org/blog/archives/2542/</link>
      <pubDate>Thu, 23 Jan 2014 11:04:34 +0800</pubDate>
      
      <guid>http://shell909090.org/blog/archives/2542/</guid>
      <description>&lt;h1&gt;说明&lt;/h1&gt;

&lt;p&gt;测试各种虚拟化系统下的虚拟机性能。&lt;/p&gt;

&lt;p&gt;测试使用sysbench。&lt;/p&gt;

&lt;p&gt;CPU采用如下指令测试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysbench --test=cpu --num-threads=2 --cpu-max-prime=50000 run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;文件IO采用如下指令测试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysbench --test=fileio --file-total-size=10G prepare
sysbench --test=fileio --file-total-size=10G --file-test-mode=rndrw --init-rng=on --max-time=300 --max-requests=0 run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;内存采用如下指令测试。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysbench --test=memory --num-threads=2 --memory-access-mode=seq run
sysbench --test=memory --num-threads=2 --memory-access-mode=rnd run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;线程采用如下指令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sysbench --test=threads --num-threads=2 run
sysbench --test=mutex --num-threads=2 --mutex-locks=1000000 run
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;裸硬盘测试采用如下指令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hdparm -tT &amp;lt;dev&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;物理机上有三个文件系统，ext4/xfs/btrfs，前两者仅做fileio测试以对比性能。&lt;/p&gt;

&lt;p&gt;另外做两个特殊文件系统对比，aufs带复制和aufs无复制。前者在只读层上准备好测试文件，而后进行随机读写测试。其中就附带了文件复制开销。后者在aufs建立后初始化测试文件，因此消除了文件复制开销。&lt;/p&gt;

&lt;p&gt;所有测试都是测试数次，取最高者（因为低者可能受到各种干扰）。一般是2-3次。&lt;/p&gt;

&lt;p&gt;物理机是一台DELL Intel 64位桌面系统，支持硬件虚拟化，有4G内存。系统采用debian jessie，测试于2014年1月17日-20日执行，内核3.12.6-2 (2013-12-29) x86_64。&lt;/p&gt;

&lt;p&gt;虚拟机lxc是使用lxc切分的一台虚拟机，没有做资源限制。&lt;/p&gt;

&lt;p&gt;虚拟机vbox是使用virtualbox切分的一台虚拟机，分配了所有CPU，打开了硬件虚拟化，分配了1G内存。&lt;/p&gt;

&lt;h1&gt;文件系统&lt;/h1&gt;

&lt;h2&gt;ext4&lt;/h2&gt;

&lt;p&gt;Operations performed: 21311 Read, 14207 Write, 45440 Other = 80958 Total Read 332.98Mb Written 221.98Mb Total transferred 554.97Mb (1.8499Mb/sec) 118.39 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0044s total number of events: 35518 total time taken by event execution: 168.4761 per-request statistics: min: 0.00ms avg: 4.74ms max: 118.67ms approx. 95 percentile: 12.48ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 35518.0000/0.00 execution time (avg/stddev): 168.4761/0.00&lt;/p&gt;

&lt;h2&gt;xfs&lt;/h2&gt;

&lt;p&gt;Operations performed: 20789 Read, 13859 Write, 44288 Other = 78936 Total Read 324.83Mb Written 216.55Mb Total transferred 541.38Mb (1.8046Mb/sec) 115.49 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0018s total number of events: 34648 total time taken by event execution: 172.0475 per-request statistics: min: 0.00ms avg: 4.97ms max: 96.11ms approx. 95 percentile: 12.30ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 34648.0000/0.00 execution time (avg/stddev): 172.0475/0.00&lt;/p&gt;

&lt;h2&gt;btrfs&lt;/h2&gt;

&lt;p&gt;Operations performed: 6180 Read, 4120 Write, 13105 Other = 23405 Total Read 96.562Mb Written 64.375Mb Total transferred 160.94Mb (549.23Kb/sec) 34.33 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0556s total number of events: 10300 total time taken by event execution: 65.8914 per-request statistics: min: 0.00ms avg: 6.40ms max: 337.28ms approx. 95 percentile: 17.01ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 10300.0000/0.00 execution time (avg/stddev): 65.8914/0.00&lt;/p&gt;

&lt;h2&gt;aufs透明&lt;/h2&gt;

&lt;p&gt;Operations performed: 5340 Read, 3560 Write, 11279 Other = 20179 Total Read 83.438Mb Written 55.625Mb Total transferred 139.06Mb (474.65Kb/sec) 29.67 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0084s total number of events: 8900 total time taken by event execution: 32.3634 per-request statistics: min: 0.00ms avg: 3.64ms max: 1037.04ms approx. 95 percentile: 0.02ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 8900.0000/0.00 execution time (avg/stddev): 32.3634/0.00&lt;/p&gt;

&lt;h2&gt;aufs非透明&lt;/h2&gt;

&lt;p&gt;Operations performed: 20320 Read, 13546 Write, 43264 Other = 77130 Total Read 317.5Mb Written 211.66Mb Total transferred 529.16Mb (1.7638Mb/sec) 112.88 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0054s total number of events: 33866 total time taken by event execution: 170.7252 per-request statistics: min: 0.00ms avg: 5.04ms max: 143.86ms approx. 95 percentile: 12.62ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 33866.0000/0.00 execution time (avg/stddev): 170.7252/0.00&lt;/p&gt;

&lt;h1&gt;物理机&lt;/h1&gt;

&lt;h2&gt;hdparm&lt;/h2&gt;

&lt;p&gt;Timing cached reads: 11980 MB in 2.00 seconds = 5992.56 MB/sec Timing buffered disk reads: 366 MB in 3.01 seconds = 121.52 MB/sec&lt;/p&gt;

&lt;h2&gt;cpu&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 51.4463s total number of events: 10000 total time taken by event execution: 102.8828 per-request statistics: min: 9.93ms avg: 10.29ms max: 36.11ms approx. 95 percentile: 11.29ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/30.00 execution time (avg/stddev): 51.4414/0.00&lt;/p&gt;

&lt;h2&gt;memory&lt;/h2&gt;

&lt;p&gt;Operations performed: 104857600 (4545662.48 ops/sec)&lt;/p&gt;

&lt;p&gt;102400.00 MB transferred (4439.12 MB/sec)&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 23.0676s total number of events: 104857600 total time taken by event execution: 34.1991 per-request statistics: min: 0.00ms avg: 0.00ms max: 18.05ms approx. 95 percentile: 0.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 52428800.0000/69790.00 execution time (avg/stddev): 17.0995/0.01&lt;/p&gt;

&lt;p&gt;Operations performed: 104857600 (5407739.22 ops/sec)&lt;/p&gt;

&lt;p&gt;102400.00 MB transferred (5281.00 MB/sec)&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 19.3903s total number of events: 104857600 total time taken by event execution: 26.8579 per-request statistics: min: 0.00ms avg: 0.00ms max: 22.46ms approx. 95 percentile: 0.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 52428800.0000/7211.00 execution time (avg/stddev): 13.4289/0.14&lt;/p&gt;

&lt;h2&gt;threads&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 1.0112s total number of events: 10000 total time taken by event execution: 2.0210 per-request statistics: min: 0.15ms avg: 0.20ms max: 11.19ms approx. 95 percentile: 0.24ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/3.00 execution time (avg/stddev): 1.0105/0.00&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 0.1665s total number of events: 2 total time taken by event execution: 0.3238 per-request statistics: min: 157.39ms avg: 161.90ms max: 166.41ms approx. 95 percentile: 10000000.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 1.0000/0.00 execution time (avg/stddev): 0.1619/0.00&lt;/p&gt;

&lt;h2&gt;nginx&lt;/h2&gt;

&lt;p&gt;Concurrency Level: 10000 Time taken for tests: 5.745 seconds Complete requests: 100000 Failed requests: 0 Write errors: 0 Total transferred: 172100000 bytes HTML transferred: 160000000 bytes Requests per second: 17404.93 &amp;#91;#/sec&amp;#93; (mean) Time per request: 574.550 &amp;#91;ms&amp;#93; (mean) Time per request: 0.057 &amp;#91;ms&amp;#93; (mean, across all concurrent requests) Transfer rate: 29251.85 [Kbytes/sec] received&lt;/p&gt;

&lt;h1&gt;lxc&lt;/h1&gt;

&lt;h2&gt;cpu&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 51.4368s total number of events: 10000 total time taken by event execution: 102.8619 per-request statistics: min: 9.92ms avg: 10.29ms max: 35.08ms approx. 95 percentile: 11.68ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/5.00 execution time (avg/stddev): 51.4310/0.00&lt;/p&gt;

&lt;h2&gt;fileio&lt;/h2&gt;

&lt;p&gt;Operations performed: 5548 Read, 3698 Write, 11776 Other = 21022 Total Read 86.688Mb Written 57.781Mb Total transferred 144.47Mb (493.07Kb/sec) 30.82 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0294s total number of events: 9246 total time taken by event execution: 84.4687 per-request statistics: min: 0.01ms avg: 9.14ms max: 394.13ms approx. 95 percentile: 36.20ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 9246.0000/0.00 execution time (avg/stddev): 84.4687/0.00&lt;/p&gt;

&lt;h2&gt;memory&lt;/h2&gt;

&lt;p&gt;Operations performed: 104857600 (4456398.83 ops/sec)&lt;/p&gt;

&lt;p&gt;102400.00 MB transferred (4351.95 MB/sec)&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 23.5297s total number of events: 104857600 total time taken by event execution: 34.8417 per-request statistics: min: 0.00ms avg: 0.00ms max: 20.22ms approx. 95 percentile: 0.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 52428800.0000/155952.00 execution time (avg/stddev): 17.4208/0.06&lt;/p&gt;

&lt;p&gt;Operations performed: 104857600 (5327923.43 ops/sec)&lt;/p&gt;

&lt;p&gt;102400.00 MB transferred (5203.05 MB/sec)&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 19.6808s total number of events: 104857600 total time taken by event execution: 27.3010 per-request statistics: min: 0.00ms avg: 0.00ms max: 16.48ms approx. 95 percentile: 0.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 52428800.0000/297738.00 execution time (avg/stddev): 13.6505/0.09&lt;/p&gt;

&lt;h2&gt;threads&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 1.2490s total number of events: 10000 total time taken by event execution: 2.4954 per-request statistics: min: 0.21ms avg: 0.25ms max: 7.39ms approx. 95 percentile: 0.28ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/7.00 execution time (avg/stddev): 1.2477/0.00&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 0.1222s total number of events: 2 total time taken by event execution: 0.2275 per-request statistics: min: 107.53ms avg: 113.77ms max: 120.02ms approx. 95 percentile: 10000000.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 1.0000/0.00 execution time (avg/stddev): 0.1138/0.01&lt;/p&gt;

&lt;h2&gt;nginx&lt;/h2&gt;

&lt;p&gt;Concurrency Level: 10000 Time taken for tests: 16.976 seconds Complete requests: 100000 Failed requests: 0 Write errors: 0 Total transferred: 1551500000 bytes HTML transferred: 1539400000 bytes Requests per second: 5890.69 &amp;#91;#/sec&amp;#93; (mean) Time per request: 1697.594 &amp;#91;ms&amp;#93; (mean) Time per request: 0.170 &amp;#91;ms&amp;#93; (mean, across all concurrent requests) Transfer rate: 89252.02 [Kbytes/sec] received&lt;/p&gt;

&lt;h1&gt;vbox&lt;/h1&gt;

&lt;h2&gt;hdparm&lt;/h2&gt;

&lt;p&gt;Timing cached reads: 10122 MB in 1.99 seconds = 5088.70 MB/sec Timing buffered disk reads: 300 MB in 3.00 seconds = 99.87 MB/sec&lt;/p&gt;

&lt;h2&gt;cpu&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 54.0469s total number of events: 10000 total time taken by event execution: 108.0595 per-request statistics: min: 9.03ms avg: 10.81ms max: 61.39ms approx. 95 percentile: 14.87ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/87.00 execution time (avg/stddev): 54.0297/0.00&lt;/p&gt;

&lt;h2&gt;fileio&lt;/h2&gt;

&lt;p&gt;Operations performed: 68153 Read, 45435 Write, 145280 Other = 258868 Total Read 1.0399Gb Written 709.92Mb Total transferred 1.7332Gb (5.916Mb/sec) 378.62 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0045s total number of events: 113588 total time taken by event execution: 177.2314 per-request statistics: min: 0.01ms avg: 1.56ms max: 579.66ms approx. 95 percentile: 13.10ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 113588.0000/0.00 execution time (avg/stddev): 177.2314/0.00&lt;/p&gt;

&lt;h2&gt;memory&lt;/h2&gt;

&lt;p&gt;一直报错，测试不出来。&lt;/p&gt;

&lt;h2&gt;threads&lt;/h2&gt;

&lt;p&gt;Test execution summary: total time: 16.0377s total number of events: 10000 total time taken by event execution: 32.0421 per-request statistics: min: 1.19ms avg: 3.20ms max: 38.39ms approx. 95 percentile: 5.74ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 5000.0000/15.00 execution time (avg/stddev): 16.0210/0.00&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 0.3023s total number of events: 2 total time taken by event execution: 0.5990 per-request statistics: min: 297.52ms avg: 299.50ms max: 301.47ms approx. 95 percentile: 10000000.00ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 1.0000/0.00 execution time (avg/stddev): 0.2995/0.00&lt;/p&gt;

&lt;h2&gt;nginx&lt;/h2&gt;

&lt;p&gt;Concurrency Level: 5000 Time taken for tests: 41.758 seconds Complete requests: 50000 Failed requests: 0 Write errors: 0 Keep-Alive requests: 0 Total transferred: 890350000 bytes HTML transferred: 884250000 bytes Requests per second: 1197.38 &amp;#91;#/sec&amp;#93; (mean) Time per request: 4175.799 &amp;#91;ms&amp;#93; (mean) Time per request: 0.835 &amp;#91;ms&amp;#93; (mean, across all concurrent requests) Transfer rate: 20821.94 [Kbytes/sec] received&lt;/p&gt;

&lt;h1&gt;vbox on lvm&lt;/h1&gt;

&lt;p&gt;在物理机上开辟一个lvm卷，然后用vmdk引用物理卷的功能挂到vbox上使用，格式化为ext4文件格式。&lt;/p&gt;

&lt;h2&gt;hdparm&lt;/h2&gt;

&lt;p&gt;Timing cached reads: 10034 MB in 1.99 seconds = 5044.30 MB/sec Timing buffered disk reads: 270 MB in 3.01 seconds = 89.73 MB/sec&lt;/p&gt;

&lt;h2&gt;fileio&lt;/h2&gt;

&lt;p&gt;Operations performed: 22765 Read, 15176 Write, 48512 Other = 86453 Total Read 355.7Mb Written 237.12Mb Total transferred 592.83Mb (1.976Mb/sec) 126.47 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0080s total number of events: 37941 total time taken by event execution: 286.5402 per-request statistics: min: 0.01ms avg: 7.55ms max: 336.67ms approx. 95 percentile: 20.49ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 37941.0000/0.00 execution time (avg/stddev): 286.5402/0.00&lt;/p&gt;

&lt;h1&gt;vbox on vdi&lt;/h1&gt;

&lt;p&gt;基本同vbox on lvm，不过使用vdi作为存储。和vbox测试相比，内部系统换为主系统同样的debian。&lt;/p&gt;

&lt;h2&gt;hdparm&lt;/h2&gt;

&lt;p&gt;Timing cached reads: 9152 MB in 1.99 seconds = 4598.80 MB/sec Timing buffered disk reads: 352 MB in 3.00 seconds = 117.16 MB/sec&lt;/p&gt;

&lt;h2&gt;fileio&lt;/h2&gt;

&lt;p&gt;Operations performed: 151020 Read, 100680 Write, 322060 Other = 573760 Total Read 2.3044Gb Written 1.5363Gb Total transferred 3.8406Gb (13.109Mb/sec) 839.00 Requests/sec executed&lt;/p&gt;

&lt;p&gt;Test execution summary: total time: 300.0011s total number of events: 251700 total time taken by event execution: 60.6296 per-request statistics: min: 0.01ms avg: 0.24ms max: 106.94ms approx. 95 percentile: 0.26ms&lt;/p&gt;

&lt;p&gt;Threads fairness: events (avg/stddev): 251700.0000/0.00 execution time (avg/stddev): 60.6296/0.00&lt;/p&gt;

&lt;h1&gt;分析&lt;/h1&gt;

&lt;h2&gt;计算&lt;/h2&gt;

&lt;p&gt;从CPU上说，vbox的消耗大约是5%，而lxc的消耗基本是0%。&lt;/p&gt;

&lt;p&gt;内存测试上，vbox无法测量。lxc的性能和物理机十分相近，两者的差异在1.5%-2%之间。&lt;/p&gt;

&lt;p&gt;在threads测试和mutex测试上，vbox显示出远远低于lxc的性能。这些基本都是内核陷入类的事务，也是预期vbox会发生性能下降的地方。&lt;/p&gt;

&lt;h2&gt;IO&lt;/h2&gt;

&lt;p&gt;从硬件设备IO上来分析，lxc和主机是共用一套物理设备的。vbox的裸设备IO比物理机低了15-18%。但是文件系统IO则表现出完全相反的景象。&lt;/p&gt;

&lt;p&gt;lxc底层使用的是btrfs，因此性能和物理机上的btrfs性能十分相近，误差在10%以内。这一性能比物理机上的ext4/xfs低了70%以上。这个表现出btrfs的性能和ext4/xfs性能的差异(注：怎么会差这么多？)。&lt;/p&gt;

&lt;p&gt;如果使用aufs的话，则要视复制特性而定。如果引发复制，性能会跌到和btrfs相近。而不引发复制的话，则和aufs下面的文件系统相近(误差在5%以内)。&lt;/p&gt;

&lt;p&gt;而vbox内只有ext4，其性能高达物理机的220%。怎么可能？&lt;/p&gt;

&lt;h2&gt;network&lt;/h2&gt;

&lt;p&gt;从nginx的rps来说，lxc的性能只有物理机的1/3，vbox的更只有7%。而且vbox连10000并发都无法支撑，只能用5000并发测试。&lt;/p&gt;

&lt;p&gt;这里固然有因为物理机目录中文件比较少的原因，但是vbox和lxc的文件数是一样的，两者性能比高达1:5是个不争的事实。&lt;/p&gt;

&lt;h2&gt;加测&lt;/h2&gt;

&lt;p&gt;因为vbox内的ext4性能太好，怀疑有鬼，所以加测了一下。果然，使用vdi后fileio性能比物理机还好（我用的是新的vdi）。这个结论在大规模读写和老的vdi上很可能退化成一点都不靠谱，否则所有的设备都应该先装一堆虚拟机做vdi。&lt;/p&gt;

&lt;h1&gt;结论&lt;/h1&gt;

&lt;h2&gt;虚拟化层级&lt;/h2&gt;

&lt;p&gt;从虚拟化的深度来说，CPU虚拟化最深，完全虚拟化次之，半虚拟化其后，硬件虚拟化和半虚拟化难分伯仲，操作系统虚拟化已经很浅了。下面是简单解说。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;CPU虚拟化：使用CPU仿真另一块CPU的每个指令。速度最慢（大约是真实CPU的1/60），可以仿真其他CPU。bochs为其代表，qemu亦支持。&lt;/li&gt;
&lt;li&gt;完全虚拟化：对另一个系统的部分核心调用进行处理。速度次之（大约一半略快）。必须是同种CPU，但可以是不同系统。vmware早期的虚拟机都是此种。&lt;/li&gt;
&lt;li&gt;半虚拟化：修改另一个系统的核心代码，使其与hypervisor交互以提高性能（大约5-10%）。必须同种CPU，但是guest系统必须可以修改（排除了windows）。Xen为其代表。&lt;/li&gt;
&lt;li&gt;硬件虚拟化：利用硬件加速完全虚拟化，使其性能接近半虚拟化，但是不需要修改内核。必须同种CPU，可以为不同系统。目前大部分虚拟机都支持。&lt;/li&gt;
&lt;li&gt;系统虚拟化：在系统上完成另一个系统的特性。必须是同种CPU同种内核，但可以是不同系统（例如debian和centos）。linux上的lxc/openvz，bsd上的jail，windows上的Virtuozzo为其代表。&lt;/li&gt;
&lt;li&gt;用户隔离：在同一个系统上，利用用户分割权限和限制配额。必须在同一个系统内。DAE为其代表。&lt;/li&gt;
&lt;li&gt;沙盒：在语言内部搭建隔离平台，对API进行鉴定和抽象。GAE为其代表。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;性能和隔离性的取舍&lt;/h2&gt;

&lt;p&gt;上面显然可见，隔离性越好，性能越差。使用硬件虚拟化后，nginx的rps只有原本的1/10。lxc的overload明明很低，但是因为用了虚拟网卡，所以rps下降到1/3。从效率上说，当然不希望为了虚拟化而消耗大量的资源。所以，如果服务原本可以用户隔离，就不要用系统虚拟化，如果可以系统虚拟化，就不要硬件虚拟化。所以在大规模使用虚拟化之前，不妨考虑一下是不是先好用户权限级隔离比较合适。&lt;/p&gt;

&lt;h2&gt;文件系统&lt;/h2&gt;

&lt;p&gt;从上面可以看出，对于lxc这种小消耗的虚拟方案，与其在意虚拟机的性能消耗，不如更在意文件系统的性能消耗。但是比较倒霉的是，lxc是不能在虚拟机内自行配置文件系统的，需要从主机内分配挂载。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ext4：适用于大部分情况，iops很高，小文件下吞吐量很不错。大部分虚拟化场景都是小系统简单服务的时候适合。&lt;/li&gt;
&lt;li&gt;xfs：适用于大设备内部的大虚拟机。每个虚机的服务复杂，或者是有大文件。&lt;/li&gt;
&lt;li&gt;btrfs：这个东西性能很低，但是写时复制的能力对快速clone很重要。适合用于产生一堆临时生成的环境，用于程序员测试或者测试工程师搭环境，测试完了就删除的。&lt;/li&gt;
&lt;li&gt;aufs：和btrfs情况差不多，快速clone不错。不过安全起见，clone后原image不可以启动实例或者修改image。在没有COW写入时性能很高，这点比btrfs好。而在COW时瞬时开销很高。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>lxc简单介绍</title>
      <link>http://shell909090.org/blog/archives/2533/</link>
      <pubDate>Thu, 02 Jan 2014 12:48:17 +0800</pubDate>
      
      <guid>http://shell909090.org/blog/archives/2533/</guid>
      <description>&lt;h1&gt;基本安装&lt;/h1&gt;

&lt;p&gt;安装lxc包。&lt;/p&gt;

&lt;p&gt;注意修改/bin/sh，链接到/bin/bash。lxc在某些版本上有一个bug，声明为/bin/sh却使用bash语法，导致不如此链接会出现错误。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wiki.debian.org/LXC&#34;&gt;lxc on debian wiki&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;镜像和设定&lt;/h1&gt;

&lt;p&gt;使用&lt;code&gt;lxc-create -n name -t template&lt;/code&gt;生成镜像。&lt;/p&gt;

&lt;p&gt;在/usr/share/lxc/templates可以看到可用的模板。&lt;/p&gt;

&lt;p&gt;在/var/cache/lxc/debian会缓存生成过程的临时文件。&lt;/p&gt;

&lt;p&gt;生成的镜像需要在镜像内安装lxc，否则无法使用lxc-execute。&lt;/p&gt;

&lt;h1&gt;资源限制&lt;/h1&gt;

&lt;p&gt;在config文件内，写入cgroup限定规则。注意，使用内存限定的话，需要在内核参数中加入cgroup_enable=memory。&lt;/p&gt;

&lt;p&gt;在debian下，可以修改/etc/default/grub文件，使用update-grub重新生成规则。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&#34;cgroup_enable=memory quiet&#34;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在config文件中可以如下限定。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc.cgroup.memory.limit_in_bytes = 512M # 限定内存
lxc.cgroup.cpuset.cpus = 0 # 限定可以使用的核
lxc.cgroup.blkio.throttle.read_bps_device = 8:0 100 # 读取速率限定
lxc.cgroup.blkio.throttle.write_bps_device = 8:0 100 # 写入速率限定
lxc.cgroup.blkio.throttle.read_iops_device = 8:0 100 # 读取频率限定
lxc.cgroup.blkio.throttle.write_iops_device = 8:0 100 # 写入频率限定
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/&#34;&gt;cgroups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/blkio-controller.txt&#34;&gt;blkio-controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/cpusets.txt&#34;&gt;cpusets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/doc/Documentation/cgroups/memory.txt&#34;&gt;memory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;执行&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;lxc-start -n name /bin/echo hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还可以用以下指令，在已经启动的container里执行进程。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc-attach -n name /bin/echo hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下指令是用来在未启动的container里执行进程的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lxc-execute -n name /bin/echo hello
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;** 注意，虽然系统初始化了所有资源，但是由于sysv-init没有执行，因此系统内初始化并没完成。这导致系统不完整，例如网络部分不可用。 **&lt;/p&gt;

&lt;h1&gt;网络隔离&lt;/h1&gt;

&lt;h2&gt;自行设定的网络&lt;/h2&gt;

&lt;p&gt;lxc采用veth技术，每次虚拟机建立时，都会产生一对veth。虚拟机内的一般叫做eth0，虚拟机外的叫做vethXXX。这张网卡可以以任何linux许可的方式进行配置。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dnsmasq所产生的dhcp封包无法被debian guest的dhcp client承认，因此无法使用dnsmasq自动部署网络。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;使用iptables配置访问&lt;/h2&gt;

&lt;p&gt;在开启br-nf后，所有bridge的包都会经过iptables的过滤。因而可以用iptables的FORWARD规则限制guest堆外网的访问。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://serverfault.com/questions/162366/iptables-bridge-and-forward-chain&#34;&gt;bridge-nf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;参考&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;http://lxc.teegra.net/&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>lxc的double NAT模式无法使用dnsmasq的分析</title>
      <link>http://shell909090.org/blog/archives/2517/</link>
      <pubDate>Mon, 25 Nov 2013 16:40:01 +0800</pubDate>
      
      <guid>http://shell909090.org/blog/archives/2517/</guid>
      <description>&lt;p&gt;系统debian testing，lxc-0.9。&lt;/p&gt;

&lt;p&gt;在笔记本上做lxc，网络是wifi，AP会drop不同MAC发出的报文，所以无法做网桥。一个办法是用ebtables做规则，我嫌麻烦。另一个是配置多头主机，double NAT。当然，还有路由器模式。不过大多数网络环境中我搞不到default gateway的权限来添加子网的路由规则。所以就选了double NAT。&lt;/p&gt;

&lt;p&gt;问题出在dnsmasq作为dhcp和dns服务器上。整个网络都搭好了，通的。完了我在host上启动dnsmasq，在guest里就是硬生生无法获得IP。&lt;/p&gt;

&lt;p&gt;首先host上vi /var/log/syslog，看到dnsmasq把dhcp offer发出去了。在guest里tcpdump，看到有报文收到。那就奇怪该死的dhclient不工作了。dhclient版本4.2.4，和主系统一致。网桥环境中这个版本可以工作。&lt;/p&gt;

&lt;p&gt;所以把网桥环境中的报文也tcpdump了一遍，加上刚刚不成功的dumpout一起看——什么都看不出来。&lt;/p&gt;

&lt;p&gt;偶然dhclient -v -d -4 eth0了一下，看到bad udp checksums，顿时一愣。跑到wireshark下面看了一下——还真是没有计算checksum。打开checksum计算可以看到double NAT里的checksum算错了。提示是maybe checksum offload。&lt;/p&gt;

&lt;p&gt;我找到这个链接：http://www.wireshark.org/docs/wsug_html_chunked/ChAdvChecksums.html&lt;/p&gt;

&lt;p&gt;里面提到，checksum可能是由网卡或驱动计算的。这就难怪——没问题的dhcp offer的udp checksum是由openwrt的网卡发出，而有问题的则是由bridge和virtual ethernet发出。&lt;/p&gt;

&lt;p&gt;那见鬼，别人是怎么成功的？&lt;/p&gt;

&lt;p&gt;一番搜索，看来lxc还不是第一个中招的：http://lists.xen.org/archives/html/xen-devel/2011-12/msg01770.html&lt;/p&gt;

&lt;p&gt;dhcp-4.2.2可以打这个补丁，使得dhcp-client不去校验udp checksum：http://pkgs.fedoraproject.org/cgit/dhcp.git/plain/dhcp-4.2.2-xen-checksum.patch?id2=HEAD&lt;/p&gt;

&lt;p&gt;当然，也有各种坑爹补丁（例如这个：http://marc.info/?l=kvm&amp;amp;m=121882968407525&amp;amp;w=2）来修复虚拟驱动上的问题，计算出正确的checksum。&lt;/p&gt;

&lt;p&gt;在debian下，他被报为bts671707(http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=671707)。已经报了一年半，尚未处理。&lt;/p&gt;

&lt;p&gt;网上很多教程能够跑通的原因，大概是因为他们的guest基于08年以后的rh系系统。rh在08年就对dhclient出了一个补丁（4.2.2那个），用于暂时修复这个问题。&lt;/p&gt;

&lt;p&gt;所有基于debian系的系统都无法从offloading不处理的系统上获得dhcp offer。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>lxc路由模式</title>
      <link>http://shell909090.org/blog/archives/2404/</link>
      <pubDate>Fri, 24 May 2013 09:25:36 +0800</pubDate>
      
      <guid>http://shell909090.org/blog/archives/2404/</guid>
      <description>&lt;h1&gt;为什么使用路由模式&lt;/h1&gt;

&lt;p&gt;lxc默认使用的是桥模式，这也是我在家里和公司里部署的模式。在这种模式下，lxc虚拟机可以直接和真实网络中的机器互相访问，就如同一台真的机器一样。路由模式则没有这个便利性。&lt;/p&gt;

&lt;p&gt;但是桥模式有个缺陷，必须能够做出桥来。我们有做不出桥来的时候么？有，如果你用笔记本，大部分AP会拒绝第二个MAC地址的包。导致网桥可以组建，却永远无法正常使用。&lt;/p&gt;

&lt;h1&gt;第一种路由模式，双重NAT简版&lt;/h1&gt;

&lt;p&gt;双重NAT可以用于几乎所有场景，并且不会带来后遗症。然而，双重NAT的问题在于，物理网络不能直接访问虚拟机。对于很多设备来说，这就失去了价值。另外说一点，之所以叫做双重NAT，是因为多数时候物理网络接到外网还需要一次NAT。&lt;/p&gt;

&lt;p&gt;lxc的双重NAT可以视为两步，建立NAT连接的虚拟网络，将lxc连接到虚拟网络。&lt;/p&gt;

&lt;p&gt;第一步比较复杂，我们先从br0的建立开始说起。首先，你需要为虚拟网络分配一个不同的保留内网网段。如果使用同样的内网网段，在ARP查询的时候会从一个端口发出超过一个的MAC回应，这就退回了桥模式。&lt;/p&gt;

&lt;p&gt;然后我们需要建立一个br0网桥作为配置的起点，对这个网桥赋予IP，配置路由和防火墙，并启动dnsmasq以便于dhcp和dns。这个模式之所以叫做简版，是因为我们先不讨论dnsmasq。&lt;/p&gt;

&lt;p&gt;假如你的lxc内网网段是192.168.66.0/24，那么你大致可以如下配置：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brctl addbr br0
ifconfig br0 192.168.66.1
route add -net 192.168.66.0/24 dev br0
iptables -A INPUT -s 192.168.66.0/24 -j ACCEPT
iptables -t nat -A POSTROUTING -s 192.168.66.0/24 -j MASQUERADE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实际上，对于任何一种网口设备，将其配置为NAT的过程都是一样的。&lt;/p&gt;

&lt;p&gt;第二步非常容易，在lxc的config文件内，指定网桥为br0就OK了。当然，作为略去dnsmasq的代价，你需要手工配置每台机器的IP地址和DNS服务器。&lt;/p&gt;

&lt;h1&gt;第二种路由模式，双重NAT&lt;/h1&gt;

&lt;p&gt;双重NAT的完整版需要在内网网口上启动dns，作为dns缓存代理和dhcp服务器。其余和第一种模式没有区别，只是你不需要手工指定IP和DNS服务器了。&lt;/p&gt;

&lt;p&gt;当然，其实任何一种网口的NAT配置都是一样的。&lt;/p&gt;

&lt;h1&gt;第三种路由模式，双边交互路由&lt;/h1&gt;

&lt;p&gt;第三种路由模式的效果最好，虚拟机和真实机可以互相访问。但是这种模式需要能够修改物理网络网关的路由表。这种模式使用主机作为路由器，中转真实网络和虚拟网络。&lt;/p&gt;

&lt;p&gt;我略去如何创造br以及如何将lxc连接到上面，这些前面有叙述。下面我简述一下双边路由最关键的几点。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;最重要的重点，就是在真实网络的网关上，将你的真实物理机在外网的IP，配置为虚拟网络的下一跳网关。例如，对于上面的例子，我们应当在网关上如此配置。&lt;/p&gt;

&lt;p&gt;route add -net 192.168.66.0/24 gw 192.168.1.4&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果不进行如此配置，物理网络所发出的包在到达网关后就不知道应当如何转发了。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在物理机上允许双边网络的所有包透过。你的包当然不能被防火墙挡掉。&lt;/li&gt;
&lt;li&gt;虚拟网络的dhcp是不会传递到外网的，因此如果打算使用dhcp，还是需要开dnsmasq。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>